<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Face Landmarks Detection using CNN | Abdur Rahman</title>
<meta name="keywords" content="DL, AI, Python, PyTorch">
<meta name="description" content="Can computers really understand the human face?">
<meta name="author" content="">
<link rel="canonical" href="https://arkalim.org/blog/face-landmarks-detection/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.2b833c6baa7a96407c2417c7a4049985b42c21cccc2472abf4603967eafd2273.css" integrity="sha256-K4M8a6p6lkB8JBfHpASZhbQsIczMJHKr9GA5Z&#43;r9InM=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://arkalim.org/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://arkalim.org/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://arkalim.org/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://arkalim.org/apple-touch-icon.png">
<link rel="mask-icon" href="https://arkalim.org/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Face Landmarks Detection using CNN" />
<meta property="og:description" content="Can computers really understand the human face?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://arkalim.org/blog/face-landmarks-detection/" />
<meta property="og:image" content="https://arkalim.org/blog/face-landmarks-detection/cover.jpg" /><meta property="article:section" content="blog" />



<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://arkalim.org/blog/face-landmarks-detection/cover.jpg" />
<meta name="twitter:title" content="Face Landmarks Detection using CNN"/>
<meta name="twitter:description" content="Can computers really understand the human face?"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Blogs",
      "item": "https://arkalim.org/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Face Landmarks Detection using CNN",
      "item": "https://arkalim.org/blog/face-landmarks-detection/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Face Landmarks Detection using CNN",
  "name": "Face Landmarks Detection using CNN",
  "description": "Can computers really understand the human face?",
  "keywords": [
    "DL", "AI", "Python", "PyTorch"
  ],
  "articleBody": "Introduction Ever wondered how Instagram applies stunning filters to your face? The software detects key points on your face and projects a mask on top. This tutorial will guide you on how to build one such software using Pytorch.\nDataset In this tutorial, we will use the official DLib Dataset which contains 6666 images of varying dimensions. Additionally, labels_ibug_300W_train.xml (comes with the dataset) contains the coordinates of 68 landmarks for each face. The script below will download the dataset and unzip it in Colab Notebook.\nif not os.path.exists('/content/ibug_300W_large_face_landmark_dataset'): !wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz !tar -xvzf 'ibug_300W_large_face_landmark_dataset.tar.gz' !rm -r 'ibug_300W_large_face_landmark_dataset.tar.gz' Here is a sample image from the dataset. We can see that the face occupies a very small fraction of the entire image. If we feed the full image to the neural network, it will also process the background (irrelevant information), making it difficult for the model to learn. Therefore, we need to crop the image and feed only the face portion.\nData Preprocessing To prevent the neural network from overfitting the training dataset, we need to randomly transform the dataset. We will apply the following operations to the training and validation dataset:\n Since the face occupies a very small portion of the entire image, crop the image and use only the face for training. Resize the cropped face into a (224x224) image. Randomly change the brightness and saturation of the resized face. Randomly rotate the face after the above three transformations. Convert the image and landmarks into torch tensors and normalize them between [-1, 1].  class Transforms(): def __init__(self): pass def rotate(self, image, landmarks, angle): angle = random.uniform(-angle, +angle) transformation_matrix = torch.tensor([ [+cos(radians(angle)), -sin(radians(angle))], [+sin(radians(angle)), +cos(radians(angle))] ]) image = imutils.rotate(np.array(image), angle) landmarks = landmarks - 0.5 new_landmarks = np.matmul(landmarks, transformation_matrix) new_landmarks = new_landmarks + 0.5 return Image.fromarray(image), new_landmarks def resize(self, image, landmarks, img_size): image = TF.resize(image, img_size) return image, landmarks def color_jitter(self, image, landmarks): color_jitter = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1) image = color_jitter(image) return image, landmarks def crop_face(self, image, landmarks, crops): left = int(crops['left']) top = int(crops['top']) width = int(crops['width']) height = int(crops['height']) image = TF.crop(image, top, left, height, width) img_shape = np.array(image).shape landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]]) landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]]) return image, landmarks def __call__(self, image, landmarks, crops): image = Image.fromarray(image) image, landmarks = self.crop_face(image, landmarks, crops) image, landmarks = self.resize(image, landmarks, (224, 224)) image, landmarks = self.color_jitter(image, landmarks) image, landmarks = self.rotate(image, landmarks, angle=10) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) return image, landmarks Dataset Class Now that we have our transformations ready, let’s write our dataset class. The labels_ibug_300W_train.xml contains the image path, landmarks and coordinates for the bounding box (for cropping the face). We will store these values in lists to access them easily during training. In this tutorial, the neural network will be trained on grayscale images.\nclass FaceLandmarksDataset(Dataset): def __init__(self, transform=None): tree = ET.parse('ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml') root = tree.getroot() self.image_filenames = [] self.landmarks = [] self.crops = [] self.transform = transform self.root_dir = 'ibug_300W_large_face_landmark_dataset' for filename in root[2]: self.image_filenames.append(os.path.join(self.root_dir, filename.attrib['file'])) self.crops.append(filename[0].attrib) landmark = [] for num in range(68): x_coordinate = int(filename[0][num].attrib['x']) y_coordinate = int(filename[0][num].attrib['y']) landmark.append([x_coordinate, y_coordinate]) self.landmarks.append(landmark) self.landmarks = np.array(self.landmarks).astype('float32') assert len(self.image_filenames) == len(self.landmarks) def __len__(self): return len(self.image_filenames) def __getitem__(self, index): image = cv2.imread(self.image_filenames[index], 0) landmarks = self.landmarks[index] if self.transform: image, landmarks = self.transform(image, landmarks, self.crops[index]) landmarks = landmarks - 0.5 return image, landmarks dataset = FaceLandmarksDataset(Transforms()) Note: landmarks = landmarks - 0.5 is done to zero-centre the landmarks as zero-centred outputs are easier for the neural network to learn.\nThe output of the dataset after preprocessing will look something like this (landmarks have been plotted on the image).\nNeural Network We will use the ResNet18 as the basic framework. We need to modify the first and last layers to suit our purpose. In the first layer, we will make the input channel count as 1 for the neural network to accept grayscale images. Similarly, in the final layer, the output channel count should equal 68 * 2 = 136 for the model to predict the (x, y) coordinates of the 68 landmarks for each face.\nclass Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name='resnet18' self.model=models.resnet18() self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features, num_classes) def forward(self, x): x=self.model(x) return x Training the Neural Network We will use the Mean Squared Error between the predicted landmarks and the true landmarks as the loss function. Keep in mind that the learning rate should be kept low to avoid exploding gradients. The network weights will be saved whenever the validation loss reaches a new minimum value. Train for at least 20 epochs to get the best performance.\nnetwork = Network() network.cuda() criterion = nn.MSELoss() optimizer = optim.Adam(network.parameters(), lr=0.0001) loss_min = np.inf num_epochs = 10 start_time = time.time() for epoch in range(1,num_epochs+1): loss_train = 0 loss_valid = 0 running_loss = 0 network.train() for step in range(1,len(train_loader)+1): images, landmarks = next(iter(train_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # clear all the gradients before calculating them optimizer.zero_grad() # find the loss for the current step loss_train_step = criterion(predictions, landmarks) # calculate the gradients loss_train_step.backward() # update the parameters optimizer.step() loss_train += loss_train_step.item() running_loss = loss_train/step print_overwrite(step, len(train_loader), running_loss, 'train') network.eval() with torch.no_grad(): for step in range(1,len(valid_loader)+1): images, landmarks = next(iter(valid_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # find the loss for the current step loss_valid_step = criterion(predictions, landmarks) loss_valid += loss_valid_step.item() running_loss = loss_valid/step print_overwrite(step, len(valid_loader), running_loss, 'valid') loss_train /= len(train_loader) loss_valid /= len(valid_loader) print('\\n--------------------------------------------------') print('Epoch: {}Train Loss: {:.4f}Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid)) print('--------------------------------------------------') if loss_valid  loss_min: loss_min = loss_valid torch.save(network.state_dict(), '/content/face_landmarks.pth') print(\"\\nMinimum Validation Loss of {:.4f}at epoch {}/{}\".format(loss_min, epoch, num_epochs)) print('Model Saved\\n') print('Training Complete') print(\"Total Elapsed Time : {}s\".format(time.time()-start_time)) Predict on Unseen Data Use the code snippet below to predict landmarks in unseen images.\nimport time import cv2 import os import numpy as np import matplotlib.pyplot as plt from PIL import Image import imutils import torch import torch.nn as nn from torchvision import models import torchvision.transforms.functional as TF ####################################################################### image_path = 'pic.jpg' weights_path = 'face_landmarks.pth' frontal_face_cascade_path = 'haarcascade_frontalface_default.xml' ####################################################################### class Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name='resnet18' self.model=models.resnet18(pretrained=False) self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features,num_classes) def forward(self, x): x=self.model(x) return x ####################################################################### face_cascade = cv2.CascadeClassifier(frontal_face_cascade_path) best_network = Network() best_network.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) best_network.eval() image = cv2.imread(image_path) grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) display_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) height, width,_ = image.shape faces = face_cascade.detectMultiScale(grayscale_image, 1.1, 4) all_landmarks = [] for (x, y, w, h) in faces: image = grayscale_image[y:y+h, x:x+w] image = TF.resize(Image.fromarray(image), size=(224, 224)) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) with torch.no_grad(): landmarks = best_network(image.unsqueeze(0)) landmarks = (landmarks.view(68,2).detach().numpy() + 0.5) * np.array([[w, h]]) + np.array([[x, y]]) all_landmarks.append(landmarks) plt.figure() plt.imshow(display_image) for landmarks in all_landmarks: plt.scatter(landmarks[:,0], landmarks[:,1], c = 'c', s = 5) plt.show()  ⚠️ The above code snippet will not work in Colab Notebook as some functionality of the OpenCV is not supported in Colab yet. To run the above cell, use your local machine.\n OpenCV Harr Cascade Classifier is used to detect faces in an image. Object detection using Haar Cascades is a machine learning-based approach where a cascade function is trained with a set of input data. OpenCV already contains many pre-trained classifiers for face, eyes, pedestrians, and many more. In our case, we will be using the face classifier for which you need to download the pre-trained classifier XML file and save it to your working directory.\nDetected faces in the input image are then cropped, resized to (224, 224) and fed to our trained neural network to predict landmarks in them.\nThe predicted landmarks in the cropped faces are then overlayed on top of the original image. The result is the image shown below. Pretty impressive, right!\nSimilarly, landmarks detection on multiple faces:\nHere, you can see that the OpenCV Harr Cascade Classifier has detected multiple faces including a false positive (a fist is predicted as a face). So, the network has plotted some landmarks on that.\nThat’s all folks! If you made it till here, hats off to you! You just trained your very own neural network to detect face landmarks in any image. Try predicting face landmarks on your webcam feed!!\nColab Notebook The complete code can be found in the interactive Colab Notebook.\n",
  "wordCount" : "1350",
  "inLanguage": "en",
  "image":"https://arkalim.org/blog/face-landmarks-detection/cover.jpg","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://arkalim.org/blog/face-landmarks-detection/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Abdur Rahman",
    "logo": {
      "@type": "ImageObject",
      "url": "https://arkalim.org/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header sticky-header">
    <nav class="nav">
        <div class="logo">
            <a href="https://arkalim.org" accesskey="h" title="Abdur Rahman (Alt + H)">Abdur Rahman</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://arkalim.org/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://arkalim.org/blog" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://arkalim.org/projects" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://arkalim.org/experience" title="Experience">
                    <span>Experience</span>
                </a>
            </li>
            <li>
                <a href="https://arkalim.org/search" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://notes.arkalim.org" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://arkalim.org">Home</a>&nbsp;»&nbsp;<a href="https://arkalim.org/blog/">Blogs</a></div>
    <h1 class="post-title">
      Face Landmarks Detection using CNN
    </h1>
    <div class="post-description">
      Can computers really understand the human face?
    </div>
    <div class="post-meta">


May 2020

</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="https://arkalim.org/blog/face-landmarks-detection/cover.jpg" alt="">
        
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">‎ Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#dataset" aria-label="Dataset">Dataset</a><ul>
                        
                <li>
                    <a href="#data-preprocessing" aria-label="Data Preprocessing">Data Preprocessing</a></li></ul>
                </li>
                <li>
                    <a href="#dataset-class" aria-label="Dataset Class">Dataset Class</a></li>
                <li>
                    <a href="#neural-network" aria-label="Neural Network">Neural Network</a></li>
                <li>
                    <a href="#training-the-neural-network" aria-label="Training the Neural Network">Training the Neural Network</a></li>
                <li>
                    <a href="#predict-on-unseen-data" aria-label="Predict on Unseen Data">Predict on Unseen Data</a></li>
                <li>
                    <a href="#thats-all-folks" aria-label="That’s all folks!">That’s all folks!</a></li>
                <li>
                    <a href="#colab-notebook" aria-label="Colab Notebook">Colab Notebook</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<p>Ever wondered how Instagram applies stunning filters to your face? The software detects key points on your face and projects a mask on top. This tutorial will guide you on how to build one such software using Pytorch.</p>
<h1 id="dataset">Dataset<a hidden class="anchor" aria-hidden="true" href="#dataset">#</a></h1>
<p>In this tutorial, we will use the official <a href="http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz">DLib Dataset</a> which contains <strong>6666 images of varying dimensions</strong>. Additionally, <em>labels_ibug_300W_train.xml</em> (comes with the dataset) contains the coordinates of <strong>68 landmarks for each face</strong>. The script below will download the dataset and unzip it in Colab Notebook.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>exists(<span style="color:#e6db74">&#39;/content/ibug_300W_large_face_landmark_dataset&#39;</span>):
    <span style="color:#960050;background-color:#1e0010">!</span>wget http:<span style="color:#f92672">//</span>dlib<span style="color:#f92672">.</span>net<span style="color:#f92672">/</span>files<span style="color:#f92672">/</span>data<span style="color:#f92672">/</span>ibug_300W_large_face_landmark_dataset<span style="color:#f92672">.</span>tar<span style="color:#f92672">.</span>gz
    <span style="color:#960050;background-color:#1e0010">!</span>tar <span style="color:#f92672">-</span>xvzf <span style="color:#e6db74">&#39;ibug_300W_large_face_landmark_dataset.tar.gz&#39;</span>    
    <span style="color:#960050;background-color:#1e0010">!</span>rm <span style="color:#f92672">-</span>r <span style="color:#e6db74">&#39;ibug_300W_large_face_landmark_dataset.tar.gz&#39;</span>
</code></pre></div><p>Here is a sample image from the dataset. We can see that the face occupies a very small fraction of the entire image. If we feed the full image to the neural network, it will also process the background (irrelevant information), making it difficult for the model to learn. Therefore, we need to crop the image and feed only the face portion.</p>
<p><img loading="lazy" src="/blog/face-landmarks-detection/img1.jpg" alt="Sample Image and Landmarks from the Dataset"  />
</p>
<h2 id="data-preprocessing">Data Preprocessing<a hidden class="anchor" aria-hidden="true" href="#data-preprocessing">#</a></h2>
<p>To prevent the neural network from overfitting the training dataset, we need to randomly transform the dataset. We will apply the following operations to the training and validation dataset:</p>
<ul>
<li>Since the face occupies a very small portion of the entire image, crop the image and use only the face for training.</li>
<li>Resize the cropped face into a (224x224) image.</li>
<li>Randomly change the brightness and saturation of the resized face.</li>
<li>Randomly rotate the face after the above three transformations.</li>
<li>Convert the image and landmarks into torch tensors and normalize them between [-1, 1].</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Transforms</span>():
    <span style="color:#66d9ef">def</span> __init__(self):
        <span style="color:#66d9ef">pass</span>
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">rotate</span>(self, image, landmarks, angle):
        angle <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>uniform(<span style="color:#f92672">-</span>angle, <span style="color:#f92672">+</span>angle)

        transformation_matrix <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([
            [<span style="color:#f92672">+</span>cos(radians(angle)), <span style="color:#f92672">-</span>sin(radians(angle))], 
            [<span style="color:#f92672">+</span>sin(radians(angle)), <span style="color:#f92672">+</span>cos(radians(angle))]
        ])

        image <span style="color:#f92672">=</span> imutils<span style="color:#f92672">.</span>rotate(np<span style="color:#f92672">.</span>array(image), angle)

        landmarks <span style="color:#f92672">=</span> landmarks <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.5</span>
        new_landmarks <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>matmul(landmarks, transformation_matrix)
        new_landmarks <span style="color:#f92672">=</span> new_landmarks <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.5</span>
        <span style="color:#66d9ef">return</span> Image<span style="color:#f92672">.</span>fromarray(image), new_landmarks

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">resize</span>(self, image, landmarks, img_size):
        image <span style="color:#f92672">=</span> TF<span style="color:#f92672">.</span>resize(image, img_size)
        <span style="color:#66d9ef">return</span> image, landmarks

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">color_jitter</span>(self, image, landmarks):
        color_jitter <span style="color:#f92672">=</span> transforms<span style="color:#f92672">.</span>ColorJitter(brightness<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>, 
                                              contrast<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>,
                                              saturation<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>, 
                                              hue<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)
        image <span style="color:#f92672">=</span> color_jitter(image)
        <span style="color:#66d9ef">return</span> image, landmarks

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">crop_face</span>(self, image, landmarks, crops):
        left <span style="color:#f92672">=</span> int(crops[<span style="color:#e6db74">&#39;left&#39;</span>])
        top <span style="color:#f92672">=</span> int(crops[<span style="color:#e6db74">&#39;top&#39;</span>])
        width <span style="color:#f92672">=</span> int(crops[<span style="color:#e6db74">&#39;width&#39;</span>])
        height <span style="color:#f92672">=</span> int(crops[<span style="color:#e6db74">&#39;height&#39;</span>])

        image <span style="color:#f92672">=</span> TF<span style="color:#f92672">.</span>crop(image, top, left, height, width)

        img_shape <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(image)<span style="color:#f92672">.</span>shape
        landmarks <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(landmarks) <span style="color:#f92672">-</span> torch<span style="color:#f92672">.</span>tensor([[left, top]])
        landmarks <span style="color:#f92672">=</span> landmarks <span style="color:#f92672">/</span> torch<span style="color:#f92672">.</span>tensor([img_shape[<span style="color:#ae81ff">1</span>], img_shape[<span style="color:#ae81ff">0</span>]])
        <span style="color:#66d9ef">return</span> image, landmarks

    <span style="color:#66d9ef">def</span> __call__(self, image, landmarks, crops):
        image <span style="color:#f92672">=</span> Image<span style="color:#f92672">.</span>fromarray(image)
        image, landmarks <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>crop_face(image, landmarks, crops)
        image, landmarks <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>resize(image, landmarks, (<span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>))
        image, landmarks <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>color_jitter(image, landmarks)
        image, landmarks <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>rotate(image, landmarks, angle<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
        
        image <span style="color:#f92672">=</span> TF<span style="color:#f92672">.</span>to_tensor(image)
        image <span style="color:#f92672">=</span> TF<span style="color:#f92672">.</span>normalize(image, [<span style="color:#ae81ff">0.5</span>], [<span style="color:#ae81ff">0.5</span>])
        <span style="color:#66d9ef">return</span> image, landmarks
</code></pre></div><h1 id="dataset-class">Dataset Class<a hidden class="anchor" aria-hidden="true" href="#dataset-class">#</a></h1>
<p>Now that we have our transformations ready, let’s write our dataset class. The <em>labels_ibug_300W_train.xml</em> contains the image path, landmarks and coordinates for the bounding box (for cropping the face). We will store these values in lists to access them easily during training. In this tutorial, the neural network will be trained on grayscale images.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">FaceLandmarksDataset</span>(Dataset):

    <span style="color:#66d9ef">def</span> __init__(self, transform<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):

        tree <span style="color:#f92672">=</span> ET<span style="color:#f92672">.</span>parse(<span style="color:#e6db74">&#39;ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml&#39;</span>)
        root <span style="color:#f92672">=</span> tree<span style="color:#f92672">.</span>getroot()

        self<span style="color:#f92672">.</span>image_filenames <span style="color:#f92672">=</span> []
        self<span style="color:#f92672">.</span>landmarks <span style="color:#f92672">=</span> []
        self<span style="color:#f92672">.</span>crops <span style="color:#f92672">=</span> []
        self<span style="color:#f92672">.</span>transform <span style="color:#f92672">=</span> transform
        self<span style="color:#f92672">.</span>root_dir <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;ibug_300W_large_face_landmark_dataset&#39;</span>
        
        <span style="color:#66d9ef">for</span> filename <span style="color:#f92672">in</span> root[<span style="color:#ae81ff">2</span>]:
            self<span style="color:#f92672">.</span>image_filenames<span style="color:#f92672">.</span>append(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(self<span style="color:#f92672">.</span>root_dir, filename<span style="color:#f92672">.</span>attrib[<span style="color:#e6db74">&#39;file&#39;</span>]))

            self<span style="color:#f92672">.</span>crops<span style="color:#f92672">.</span>append(filename[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>attrib)

            landmark <span style="color:#f92672">=</span> []
            <span style="color:#66d9ef">for</span> num <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">68</span>):
                x_coordinate <span style="color:#f92672">=</span> int(filename[<span style="color:#ae81ff">0</span>][num]<span style="color:#f92672">.</span>attrib[<span style="color:#e6db74">&#39;x&#39;</span>])
                y_coordinate <span style="color:#f92672">=</span> int(filename[<span style="color:#ae81ff">0</span>][num]<span style="color:#f92672">.</span>attrib[<span style="color:#e6db74">&#39;y&#39;</span>])
                landmark<span style="color:#f92672">.</span>append([x_coordinate, y_coordinate])
            self<span style="color:#f92672">.</span>landmarks<span style="color:#f92672">.</span>append(landmark)

        self<span style="color:#f92672">.</span>landmarks <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(self<span style="color:#f92672">.</span>landmarks)<span style="color:#f92672">.</span>astype(<span style="color:#e6db74">&#39;float32&#39;</span>)     

        <span style="color:#66d9ef">assert</span> len(self<span style="color:#f92672">.</span>image_filenames) <span style="color:#f92672">==</span> len(self<span style="color:#f92672">.</span>landmarks)

    <span style="color:#66d9ef">def</span> __len__(self):
        <span style="color:#66d9ef">return</span> len(self<span style="color:#f92672">.</span>image_filenames)

    <span style="color:#66d9ef">def</span> __getitem__(self, index):
        image <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>imread(self<span style="color:#f92672">.</span>image_filenames[index], <span style="color:#ae81ff">0</span>)
        landmarks <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>landmarks[index]
        
        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>transform:
            image, landmarks <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transform(image, landmarks, self<span style="color:#f92672">.</span>crops[index])

        landmarks <span style="color:#f92672">=</span> landmarks <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.5</span>

        <span style="color:#66d9ef">return</span> image, landmarks

dataset <span style="color:#f92672">=</span> FaceLandmarksDataset(Transforms())
</code></pre></div><p><strong>Note:</strong> <code>landmarks = landmarks - 0.5</code> is done to zero-centre the landmarks as zero-centred outputs are easier for the neural network to learn.</p>
<p>The output of the dataset after preprocessing will look something like this (landmarks have been plotted on the image).</p>
<p><img loading="lazy" src="/blog/face-landmarks-detection/img2.jpg" alt="Preprocessed Data Sample"  />
</p>
<h1 id="neural-network">Neural Network<a hidden class="anchor" aria-hidden="true" href="#neural-network">#</a></h1>
<p>We will use the ResNet18 as the basic framework. We need to modify the first and last layers to suit our purpose. In the first layer, we will make the input channel count as 1 for the neural network to accept grayscale images. Similarly, in the final layer, the output channel count should equal <strong>68 * 2 = 136</strong> for the model to predict the (x, y) coordinates of the 68 landmarks for each face.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Network</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self,num_classes<span style="color:#f92672">=</span><span style="color:#ae81ff">136</span>):
        super()<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;resnet18&#39;</span>
        self<span style="color:#f92672">.</span>model<span style="color:#f92672">=</span>models<span style="color:#f92672">.</span>resnet18()
        self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>conv1<span style="color:#f92672">=</span>nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">64</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
        self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>fc<span style="color:#f92672">=</span>nn<span style="color:#f92672">.</span>Linear(self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>fc<span style="color:#f92672">.</span>in_features, num_classes)
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        x<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>model(x)
        <span style="color:#66d9ef">return</span> x
</code></pre></div><h1 id="training-the-neural-network">Training the Neural Network<a hidden class="anchor" aria-hidden="true" href="#training-the-neural-network">#</a></h1>
<p>We will use the Mean Squared Error between the predicted landmarks and the true landmarks as the loss function. Keep in mind that the learning rate should be kept low to avoid exploding gradients. The network weights will be saved whenever the validation loss reaches a new minimum value. Train for at least 20 epochs to get the best performance.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">network <span style="color:#f92672">=</span> Network()
network<span style="color:#f92672">.</span>cuda()    

criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MSELoss()
optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>Adam(network<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0001</span>)

loss_min <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>inf
num_epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>

start_time <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
<span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>,num_epochs<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>):
    
    loss_train <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    loss_valid <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    running_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    
    network<span style="color:#f92672">.</span>train()
    <span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>,len(train_loader)<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>):
    
        images, landmarks <span style="color:#f92672">=</span> next(iter(train_loader))
        
        images <span style="color:#f92672">=</span> images<span style="color:#f92672">.</span>cuda()
        landmarks <span style="color:#f92672">=</span> landmarks<span style="color:#f92672">.</span>view(landmarks<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>),<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>cuda() 
        
        predictions <span style="color:#f92672">=</span> network(images)
        
        <span style="color:#75715e"># clear all the gradients before calculating them</span>
        optimizer<span style="color:#f92672">.</span>zero_grad()
        
        <span style="color:#75715e"># find the loss for the current step</span>
        loss_train_step <span style="color:#f92672">=</span> criterion(predictions, landmarks)
        
        <span style="color:#75715e"># calculate the gradients</span>
        loss_train_step<span style="color:#f92672">.</span>backward()
        
        <span style="color:#75715e"># update the parameters</span>
        optimizer<span style="color:#f92672">.</span>step()
        
        loss_train <span style="color:#f92672">+=</span> loss_train_step<span style="color:#f92672">.</span>item()
        running_loss <span style="color:#f92672">=</span> loss_train<span style="color:#f92672">/</span>step
        
        print_overwrite(step, len(train_loader), running_loss, <span style="color:#e6db74">&#39;train&#39;</span>)
        
    network<span style="color:#f92672">.</span>eval() 
    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
        
        <span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>,len(valid_loader)<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>):
            
            images, landmarks <span style="color:#f92672">=</span> next(iter(valid_loader))
        
            images <span style="color:#f92672">=</span> images<span style="color:#f92672">.</span>cuda()
            landmarks <span style="color:#f92672">=</span> landmarks<span style="color:#f92672">.</span>view(landmarks<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>),<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>cuda()
        
            predictions <span style="color:#f92672">=</span> network(images)

            <span style="color:#75715e"># find the loss for the current step</span>
            loss_valid_step <span style="color:#f92672">=</span> criterion(predictions, landmarks)

            loss_valid <span style="color:#f92672">+=</span> loss_valid_step<span style="color:#f92672">.</span>item()
            running_loss <span style="color:#f92672">=</span> loss_valid<span style="color:#f92672">/</span>step

            print_overwrite(step, len(valid_loader), running_loss, <span style="color:#e6db74">&#39;valid&#39;</span>)
    
    loss_train <span style="color:#f92672">/=</span> len(train_loader)
    loss_valid <span style="color:#f92672">/=</span> len(valid_loader)
    
    print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">--------------------------------------------------&#39;</span>)
    print(<span style="color:#e6db74">&#39;Epoch: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">  Train Loss: </span><span style="color:#e6db74">{:.4f}</span><span style="color:#e6db74">  Valid Loss: </span><span style="color:#e6db74">{:.4f}</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">.</span>format(epoch, loss_train, loss_valid))
    print(<span style="color:#e6db74">&#39;--------------------------------------------------&#39;</span>)
    
    <span style="color:#66d9ef">if</span> loss_valid <span style="color:#f92672">&lt;</span> loss_min:
        loss_min <span style="color:#f92672">=</span> loss_valid
        torch<span style="color:#f92672">.</span>save(network<span style="color:#f92672">.</span>state_dict(), <span style="color:#e6db74">&#39;/content/face_landmarks.pth&#39;</span>) 
        print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Minimum Validation Loss of </span><span style="color:#e6db74">{:.4f}</span><span style="color:#e6db74"> at epoch </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(loss_min, epoch, num_epochs))
        print(<span style="color:#e6db74">&#39;Model Saved</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
     
print(<span style="color:#e6db74">&#39;Training Complete&#39;</span>)
print(<span style="color:#e6db74">&#34;Total Elapsed Time : </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> s&#34;</span><span style="color:#f92672">.</span>format(time<span style="color:#f92672">.</span>time()<span style="color:#f92672">-</span>start_time))
</code></pre></div><h1 id="predict-on-unseen-data">Predict on Unseen Data<a hidden class="anchor" aria-hidden="true" href="#predict-on-unseen-data">#</a></h1>
<p>Use the code snippet below to predict landmarks in unseen images.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> time
<span style="color:#f92672">import</span> cv2
<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
<span style="color:#f92672">from</span> PIL <span style="color:#f92672">import</span> Image
<span style="color:#f92672">import</span> imutils

<span style="color:#f92672">import</span> torch
<span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
<span style="color:#f92672">from</span> torchvision <span style="color:#f92672">import</span> models
<span style="color:#f92672">import</span> torchvision.transforms.functional <span style="color:#66d9ef">as</span> TF
<span style="color:#75715e">#######################################################################</span>
image_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;pic.jpg&#39;</span>
weights_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;face_landmarks.pth&#39;</span>
frontal_face_cascade_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;haarcascade_frontalface_default.xml&#39;</span>
<span style="color:#75715e">#######################################################################</span>
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Network</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self,num_classes<span style="color:#f92672">=</span><span style="color:#ae81ff">136</span>):
        super()<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;resnet18&#39;</span>
        self<span style="color:#f92672">.</span>model<span style="color:#f92672">=</span>models<span style="color:#f92672">.</span>resnet18(pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
        self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>conv1<span style="color:#f92672">=</span>nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">64</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
        self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>fc<span style="color:#f92672">=</span>nn<span style="color:#f92672">.</span>Linear(self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>fc<span style="color:#f92672">.</span>in_features,num_classes)
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        x<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>model(x)
        <span style="color:#66d9ef">return</span> x

<span style="color:#75715e">#######################################################################</span>
face_cascade <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>CascadeClassifier(frontal_face_cascade_path)

best_network <span style="color:#f92672">=</span> Network()
best_network<span style="color:#f92672">.</span>load_state_dict(torch<span style="color:#f92672">.</span>load(weights_path, map_location<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#39;cpu&#39;</span>))) 
best_network<span style="color:#f92672">.</span>eval()

image <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>imread(image_path)
grayscale_image <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>cvtColor(image, cv2<span style="color:#f92672">.</span>COLOR_BGR2GRAY)
display_image <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>cvtColor(image, cv2<span style="color:#f92672">.</span>COLOR_BGR2RGB)
height, width,_ <span style="color:#f92672">=</span> image<span style="color:#f92672">.</span>shape

faces <span style="color:#f92672">=</span> face_cascade<span style="color:#f92672">.</span>detectMultiScale(grayscale_image, <span style="color:#ae81ff">1.1</span>, <span style="color:#ae81ff">4</span>)

all_landmarks <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> (x, y, w, h) <span style="color:#f92672">in</span> faces:
    image <span style="color:#f92672">=</span> grayscale_image[y:y<span style="color:#f92672">+</span>h, x:x<span style="color:#f92672">+</span>w]
    image <span style="color:#f92672">=</span> TF<span style="color:#f92672">.</span>resize(Image<span style="color:#f92672">.</span>fromarray(image), size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>))
    image <span style="color:#f92672">=</span> TF<span style="color:#f92672">.</span>to_tensor(image)
    image <span style="color:#f92672">=</span> TF<span style="color:#f92672">.</span>normalize(image, [<span style="color:#ae81ff">0.5</span>], [<span style="color:#ae81ff">0.5</span>])

    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
        landmarks <span style="color:#f92672">=</span> best_network(image<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)) 

    landmarks <span style="color:#f92672">=</span> (landmarks<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">68</span>,<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>numpy() <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.5</span>) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>array([[w, h]]) <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>array([[x, y]])
    all_landmarks<span style="color:#f92672">.</span>append(landmarks)

plt<span style="color:#f92672">.</span>figure()
plt<span style="color:#f92672">.</span>imshow(display_image)
<span style="color:#66d9ef">for</span> landmarks <span style="color:#f92672">in</span> all_landmarks:
    plt<span style="color:#f92672">.</span>scatter(landmarks[:,<span style="color:#ae81ff">0</span>], landmarks[:,<span style="color:#ae81ff">1</span>], c <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;c&#39;</span>, s <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>)

plt<span style="color:#f92672">.</span>show()
</code></pre></div><blockquote>
<p>⚠️ The above code snippet will not work in Colab Notebook as some functionality of the OpenCV is not supported in Colab yet. To run the above cell, use your local machine.</p>
</blockquote>
<p><strong>OpenCV Harr Cascade Classifier</strong> is used to detect faces in an image. Object detection using Haar Cascades is a machine learning-based approach where a cascade function is trained with a set of input data. OpenCV already contains many pre-trained classifiers for face, eyes, pedestrians, and many more. In our case, we will be using the face classifier for which you need to download the pre-trained classifier XML file and save it to your working directory.</p>
<p><img loading="lazy" src="/blog/face-landmarks-detection/img3.jpg" alt="Face Detection"  />
</p>
<p>Detected faces in the input image are then cropped, resized to <strong>(224, 224)</strong> and fed to our trained neural network to predict landmarks in them.</p>
<p><img loading="lazy" src="/blog/face-landmarks-detection/img4.jpg" alt="Landmarks Detection on the Cropped Face "  />
</p>
<p>The predicted landmarks in the cropped faces are then overlayed on top of the original image. The result is the image shown below. Pretty impressive, right!</p>
<p><img loading="lazy" src="/blog/face-landmarks-detection/cover.jpg" alt="Final Result"  />
</p>
<p>Similarly, landmarks detection on multiple faces:</p>
<p><img loading="lazy" src="/blog/face-landmarks-detection/img5.jpg" alt="Detection on multiple faces"  />
</p>
<p>Here, you can see that the OpenCV Harr Cascade Classifier has detected multiple faces including a false positive (a fist is predicted as a face). So, the network has plotted some landmarks on that.</p>
<h1 id="thats-all-folks">That’s all folks!<a hidden class="anchor" aria-hidden="true" href="#thats-all-folks">#</a></h1>
<p>If you made it till here, hats off to you! You just trained your very own neural network to detect face landmarks in any image. Try predicting face landmarks on your webcam feed!!</p>
<h1 id="colab-notebook">Colab Notebook<a hidden class="anchor" aria-hidden="true" href="#colab-notebook">#</a></h1>
<p>The complete code can be found in the interactive <a href="https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb">Colab Notebook</a>.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://arkalim.org/tags/dl/">DL</a></li>
      <li><a href="https://arkalim.org/tags/ai/">AI</a></li>
      <li><a href="https://arkalim.org/tags/python/">Python</a></li>
      <li><a href="https://arkalim.org/tags/pytorch/">PyTorch</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://arkalim.org">Abdur Rahman</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
